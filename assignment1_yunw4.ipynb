{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "fb64f22f4b9d6a57ea7261cf771e646e223fad9c85d255dda4a7efa4"
   },
   "source": [
    "# COMP90042 Assignment #1: Sentiment analysis for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "signature": "dbe5c8ee5ef597fc2680bec06e686a52a01edd44992269bd65c07d09"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-8166fd4864cd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-8166fd4864cd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Student Name: Yun Wang\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Student Name: Yun Wang\n",
    "Student ID: 672323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "5653cf33f236b021076096666f0eaca598a8b6ea818c0d2a6425a15b"
   },
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "eac8f7335062ea69220caf0a93873dbcb9121a0c0d8bafc349dbbfff"
   },
   "source": [
    "<b>Due date</b>: 5pm, Mon April 12\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this ipython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day, no late submissions after the first week\n",
    "\n",
    "<b>Marks</b>: 25% of mark for class\n",
    "\n",
    "<b>Overview</b>: For this project, you'll be building a 3-way polarity classification system for tweets, using a logistic regression classifier, BOW features, as well as polarity lexicons built from external sources. A key focus of this project is critical analysis and experimental evaluation, for which you will need to report on the relative merits of various options. \n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Sci-kit Learn, and Gemsim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python build-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. You are encouraged to use the iPython notebooks released for this class as well as other online documentation to guide your responses, but you should not copy directly from any source. The only other data you will need is three sets of tagged tweets, the first two of which (training and dev) were released at the same time as this notebook, and a third set (test) which will be made available about a week before the assignment is due, see Final Testing below. This data comes from the recent SemEval 2016 shared task. Do not distribute this data indiscriminately (i.e. put it on a public website), you should use it only for this assignment, and delete it afterwards. The corpus is comprised of unfiltered text from the web, and may include offensive or objectionable material. This reflects the general composition of the web and the general challenges present in web based text analysis. The University of Melbourne takes no responsibility for opinions expressed in the corpus, nor takes any responsibility for offence caused by these documents.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time (less than 10 minutes on a lab desktop), and you must follow all instructions provided below, including specific implementation requirements. You will be marked not only on the correctness of your methods, but also on your explanation and analysis. Please do not change any of instruction text in the notebook. Where applicable, leave the output cells in the code, particularly when you are commenting on that output. You should add your answers and code by inserting a markdown cell between every major function or other block of code explaining its purpose or anywhere a result needs to be discussed (see the class notebooks for examples). Note that even if you do something wrong, you might get partial credit if you explain it enough that we can follow your reasoning, whereas a fully correct assignment with no text commentary will not receive a passing score. You will not be marked directly on the performance of your final classifier, but each of the steps you take to build it should be reasonable and well-defended.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this project, and we encourage you to discuss it in general terms with other students. However, it is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "55678c4756c3c6d4aa908b80503b61a10c423f53cd814b52ad443b54"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "9c1902745267a3e3d99862e5ec6431ab9f901f75baa61ef5a024a968"
   },
   "source": [
    "<b>Instructions</b>: Your first task is to carry out preprocessing on the tweets. Use the code below as a starter. Each line of the input files is a json including the tweet and the label (and the tweet id), this code just loads them into a list without any preprocessing. Note that for the labels, 1 = positive, 0 = neutral, -1 = negative. Here is a list of things your preprocessing code must do:\n",
    "\n",
    "<ul>\n",
    "<li>Segment into sentences: Use NLTK punkt sentence segmenter</li>\n",
    "<li>Tokenize sentences: Use the NLTK regex WordPunct tokenizer</li>\n",
    "<li>Lowercase all words</li>\n",
    "<li>Remove Twitter usernames: Usernames on twitter begin with @</li>\n",
    "<li>Remove URLs: URLs start with http</li> \n",
    "<li>Remove any hashtags from their original location in the tweet, tokenize them, and add them as a separate sentences with the hash tag removed: for tokenization, use capitalized letters when they occur (e.g. #RefugeesWelcome -> Refugees Welcome), or when there is no capitalization (#refugeeswelcome -> refugees welcome) use the MaxMatch algorithm and the list of English words included in NLTK (nltk.corpus.words.words()). Two notes about the English word list: 1. you should convert it to a python set before you use it (sets are hashed, so you get much quicker lookup) 2. It contains only base forms, so you will need to lemmatize words before you look them up.</li>\n",
    "</ul>\n",
    "\n",
    "You can do these in almost any order you like, but it may be useful to do the main segmentation/tokenization last (or almost last), since for the other tasks it is easier to deal with the raw string rather than a list of tokens. The use of regular expressions is recommended, but not required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "signature": "d206ed2a6dd099cad03569db61a8b4b5de7e6ec1c4590e340a588a93"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "word_set=set(nltk.corpus.words.words())\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "# count the number of tweets required to print\n",
    "num_tweet_print=1\n",
    "# a list used to hold the original tweets(in json format)\n",
    "original_tweet=[]\n",
    "'''\n",
    "Preprocess a single tweet.\n",
    "The method takes a single tweet as input, remove all the '@'s and urls at first, \n",
    "then find all the hashtags in this tweet and store them into a list. Then the hashtags \n",
    "are tokenized based on whether there is capticalization. If yes, it is splitted using the capital \n",
    "letters, if not, it is splitted using maxMatch algorithm-- lemmatize the substring first, then \n",
    "look it up in the word_set(nltk.corpus.words.words()). After this, all the hashtags are removed \n",
    "from tweet and the tweet is tokenized into sentences then words. Finally, the list of words inside\n",
    "the hashtag is added to the end of the list of words from tweet and returned.\n",
    "'''\n",
    "def preprocess(ori_tweet):\n",
    "    global num_tweet_print\n",
    "    # get rid of all the \"@\"s and urls\n",
    "    tweet = re.sub(\"@[^ ]+\", \"\", ori_tweet).strip()\n",
    "    tweet=re.sub(\"http[^ ]+\",\"\",tweet).strip()\n",
    "    # Find all the hashtags and put them in a list called hashtag\n",
    "    hashtag = re.findall('#[^ ]*', tweet)\n",
    "    tag_list=[]\n",
    "    if hashtag:\n",
    "        for tag in hashtag:\n",
    "            #remove the \"#\" from one hashtag\n",
    "            tag=re.sub('#','',tag)\n",
    "            # if contains capital letters\n",
    "            if re.search('.*[A-Z].*',tag):\n",
    "                tag_list=tag_list+re.findall('[A-Z][^A-Z]*',tag)\n",
    "\n",
    "            else:\n",
    "                 # if not contain capital letters use a maxmatch algorithm to find the words\n",
    "                i=0\n",
    "                while i<len(tag):\n",
    "                    for j in range(len(tag),i,-1):\n",
    "                        #lemmatize words first\n",
    "                        lemma=lemmatize(tag[i:j])\n",
    "                        if lemma in word_set:\n",
    "                            tag_list.append(tag[i:j])\n",
    "                            i=j-1\n",
    "                    i+=1\n",
    "    if tag_list:\n",
    "        for i in range(len(tag_list)):\n",
    "            tag_list[i]=tag_list[i].lower()\n",
    "    #remove all the hashtags\n",
    "    tweet = re.sub('#[^ ]*','',tweet).lower()\n",
    "    #split tweet into sentences\n",
    "    tweet = sent_segmenter.tokenize(tweet)\n",
    "    #put the words in a tweet into a words list\n",
    "    words = []\n",
    "    for sentence in tweet:\n",
    "        words= words+ word_tokenizer.tokenize(sentence)\n",
    "    #if the tweet has hashtags, add them to the words list too\n",
    "    if tag_list:\n",
    "        words=words+tag_list\n",
    "    if len(tag_list)>1:\n",
    "        if num_tweet_print<11:\n",
    "            print num_tweet_print\n",
    "            print ori_tweet\n",
    "            print words\n",
    "            num_tweet_print+=1\n",
    "\n",
    "    return words\n",
    "\n",
    "'''\n",
    "Given the file name, pre-process the file and return a list of preprocessed tweet \n",
    "and label lists\n",
    "'''\n",
    "def preprocess_file(filename):\n",
    "    tweets = []\n",
    "    labels=[]\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        original_tweet.append(tweet_dict)\n",
    "        tweets.append(preprocess(tweet_dict[\"text\"]))\n",
    "        labels.append(int(tweet_dict[\"label\"]))\n",
    "    return tweets,labels\n",
    "'''\n",
    "lemmatize a word. First treat the word as a verb, if not then treat the word as a noun\n",
    "'''\n",
    "def lemmatize(word):\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "7dc1fa62f9e403c31f91caba69d41d98516063cde6b0d1ced8de92df"
   },
   "source": [
    "<b>Instructions</b>: Once your basic preprocessing module is working, run it on the training set and have it print out 10 examples where your system identified a hashtag with more than one word inside; print out both the original tweet string as well as result after preprocessing. It's okay if you have to duplicate some code from above to do this. Point out any errors you see in the preprocessing, and discuss possible solutions; these can be related to the hashtags, or any other errors you see. You do not have to fix the errors unless they actually indicate a actual bug in your code (at which point you should go back to the previous section, fix the code, and print out the samples again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "signature": "81d5303fd413fafb373f77293320f3b0aca065df23c6f4a55043585e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @majornelson @Microsoft\n",
      "[u'if', u'i', u'make', u'a', u'game', u'as', u'a', u'universal', u'app', u'.', u'will', u'owners', u'be', u'able', u'to', u'download', u'and', u'play', u'it', u'in', u'november', u'?', u'windows', u'x', u'box', u'one']\n",
      "2\n",
      "@MikeWolf1980 @Microsoft I will be downgrading and let #Windows10 be out for almost the 1st yr b4 trying it again. #Windows10fail\n",
      "[u'i', u'will', u'be', u'downgrading', u'and', u'let', u'be', u'out', u'for', u'almost', u'the', u'1st', u'yr', u'b4', u'trying', u'it', u'again', u'.', u'windows10', u'windows10fail']\n",
      "3\n",
      "For the 1st time @Skype has a \"High Startup impact\" Does anyone at @Microsoft have a clue?#Windows10Fail http://t.co/loO3yd5rwe\n",
      "[u'for', u'the', u'1st', u'time', u'has', u'a', u'\"', u'high', u'startup', u'impact', u'\"', u'does', u'anyone', u'at', u'have', u'a', u'clue', u'?', u'windows10', u'fail']\n",
      "4\n",
      "#teens @BillGates 1st company failed miserably. When Gates & @PaulGAllen tried to sell the product it wouldn't work #nevergiveup @Microsoft\n",
      "[u'1st', u'company', u'failed', u'miserably', u'.', u'when', u'gates', u'&', u'tried', u'to', u'sell', u'the', u'product', u'it', u'wouldn', u\"'\", u't', u'work', u'teens', u'never', u'give', u'up']\n",
      "5\n",
      "#Vote for @AIESEC to become the 10th Global non profit partner of @Microsoft for us to #UpgradeYourWorld together. @AIESECGermany\n",
      "[u'for', u'to', u'become', u'the', u'10th', u'global', u'non', u'profit', u'partner', u'of', u'for', u'us', u'to', u'together', u'.', u'vote', u'upgrade', u'your', u'world']\n",
      "6\n",
      "Top 5 most searched for Back-to-School topics -- the list may surprise you http://t.co/Xj21uMVo0p  @bing @MSFTnews #backtoschool @Microsoft\n",
      "[u'top', u'5', u'most', u'searched', u'for', u'back', u'-', u'to', u'-', u'school', u'topics', u'--', u'the', u'list', u'may', u'surprise', u'you', u'back', u'to', u'school']\n",
      "7\n",
      "@taehongmin1 We have an IOT workshop by @Microsoft at 11PM on the Friday - definitely worth going for inspiration! #HackThePlanet\n",
      "[u'we', u'have', u'an', u'iot', u'workshop', u'by', u'at', u'11pm', u'on', u'the', u'friday', u'-', u'definitely', u'worth', u'going', u'for', u'inspiration', u'!', u'hack', u'the', u'planet']\n",
      "8\n",
      "@ForbesRussia #MBA #casestudy Namaste 2 #google and @Microsoft's CEOs, but #Multiculturals mttr!May B the era of Bad Translations wd B Over?\n",
      "[u'namaste', u'2', u'and', u'ceos', u',', u'but', u'mttr', u'!', u'may', u'b', u'the', u'era', u'of', u'bad', u'translations', u'wd', u'b', u'over', u'?', u'm', u'b', u'a', u'cases', u'tu', u'd', u'y', u'goo', u'g', u'l', u'e', u'multiculturals']\n",
      "9\n",
      "After 75 minutes of being on hold with @Microsoft in India 1-800-936-5700 \"Adrian\" wants to transfer my call again(3rd time) #Windows10Fail\n",
      "[u'after', u'75', u'minutes', u'of', u'being', u'on', u'hold', u'with', u'in', u'india', u'1', u'-', u'800', u'-', u'936', u'-', u'5700', u'\"', u'adrian', u'\"', u'wants', u'to', u'transfer', u'my', u'call', u'again', u'(', u'3rd', u'time', u')', u'windows10', u'fail']\n",
      "10\n",
      "We're excited to learn about #cloud #analytics from @Microsoft tomorrow! Join us https://t.co/p0bMREBBHC #tech #rva http://t.co/1XHmPdSvzq\n",
      "[u'we', u\"'\", u're', u'excited', u'to', u'learn', u'about', u'from', u'tomorrow', u'!', u'join', u'us', u'cloud', u'analytics', u'tech', u'r', u'v', u'a']\n"
     ]
    }
   ],
   "source": [
    "trn_tweets,trn_labels=preprocess_file('train.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>My analysis</b>:\n",
    "From the above output, we can see the following errors:\n",
    "1. When the hashtag has several words in it, but does not have enough capital letters to separate them, like  #Windows10fail, our code only split this hashtag based on \"W\", thus returning Windows10fail as a single word. \n",
    "2. When the hashtag is an abbreviation, consisting of only capital letters, like #MBA, which should be recognized as a single word, but our code would split it into \"M\",\"B\",\"A\"\n",
    "3. My implementation of maxMatch matches the hashtag from left to right, for hashtags like #casestudy, the algorithm would first match 'cases' with 'case', and 'tudy' is left, no word can be matched with 'tudy'\n",
    "4. Punctuations are useless most of the times but are not removed.\n",
    "\n",
    "\n",
    "To solve problem 1 and 2, we have to include a more delicate method to tell if the hashtag is an abbreviation, or if there is not enough capital letters to split the string. In addition, many words are not included in the list of English words from NLTK, thus we may need a more comprehensive lexicon. For error 3, a more intelligent maxMatch algorithm is needed, which will match the longest word in the string first, and is able to tell whether 's' or 'ed' come from the tense of the word. For error 4, we just need to include an extra step to remove punctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "ec5ec021ac23aa5b89b40b7df4d6c8031e8e07cae5e2eebf80d873d0"
   },
   "source": [
    "<b>Instructions</b>: The next step will be to convert each of your preprocessed tweets into a feature dictionary, that is, a python dictionary where each entry corresponds to a feature and its value. At this stage, you should just build a bag-of-word feature dict, though you must allow for two possible options: one is to remove stopwords (using the NLTK stopword list), and the other is to remove words appearing <em>less</em> than n times across the entire training set (n<=0 should have no effect). The outer function (convert_to_feature dicts) should take the list of tweets resulting from the preprocess_file, and return a list of feature dictionaries in the same order (so they correspond to the label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "signature": "dfc552b1cad57233536271c6d32c3f09dba50ff56286f6a194e25f90"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Method convert_to_feature_dicts\n",
    "Take the preprocessed list of tweets as input, remove_stop_words indicates whether to remove\n",
    "stop words, and n denotes whether to remove words appearing less than n times across the \n",
    "entire training set. \n",
    "Return a feature dictionary. \n",
    "\n",
    "First construct a dictionary using all the words in the training set, which is used to \n",
    "identify the words appearing less than n times. These words are stored into small_set. \n",
    "If remove_stop_words==True and n >0, then only words not in small_set and stop_word_set \n",
    "will be added to the feature dic.\n",
    "'''\n",
    "\n",
    "def convert_to_feature_dicts(tweets,remove_stop_words,n):\n",
    "    feature_dicts = []\n",
    "    if remove_stop_words:\n",
    "        stop_word_set=set(stopwords.words('english'))\n",
    "    if n>0:\n",
    "        small_set=set()\n",
    "        whole_feature_dic={}\n",
    "        for tweet in tweets:\n",
    "            for w in tweet:\n",
    "                whole_feature_dic[w]=whole_feature_dic.get(w,0)+1\n",
    "\n",
    "        for w in whole_feature_dic:\n",
    "            if whole_feature_dic[w]<=n:\n",
    "                small_set.add(w)\n",
    "\n",
    "    for tweet in tweets:\n",
    "        # build feature dictionary for tweet\n",
    "        feature_dict={}\n",
    "        for w in tweet:\n",
    "\n",
    "            if remove_stop_words and n<=0:\n",
    "                if w not in stop_word_set:\n",
    "                    feature_dict[w]=feature_dict.get(w,0)+1\n",
    "            elif remove_stop_words and n>0:\n",
    "                if w not in stop_word_set and w not in small_set:\n",
    "                    feature_dict[w]=feature_dict.get(w,0)+1\n",
    "            elif n>0:\n",
    "                if w not in small_set:\n",
    "                    feature_dict[w]=feature_dict.get(w,0)+1\n",
    "            else:\n",
    "                feature_dict[w]=feature_dict.get(w,0)+1\n",
    "\n",
    "        feature_dicts.append(feature_dict)\n",
    "\n",
    "    return feature_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "932fa4ffe864b9155e1d31b112ec463cd0c11684f77071cfeaea72f7"
   },
   "source": [
    "## Tuning and classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "00083e185628eaf2ebd60c081534bb750f4755b7a683f5c2dcea11ce"
   },
   "source": [
    "<b>Instructions</b>: Using the functions you've written, you should produce lists of feature dictionaries for both training and development sets; for the training set, remove stopwords and all words that appear only once (do <em>not</em> this for the dev set). Using scikit learn, convert the data to the sparse representation used for training classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "signature": "20ef700ae3f07171f04d1e3d5cdf837b36e17f9dcf1f80881b767a0d"
   },
   "outputs": [],
   "source": [
    "\n",
    "#proprocess training data\n",
    "trn_tweets,trn_labels=preprocess_file('train.json')\n",
    "# convert training data into feature dictionaries\n",
    "trn_feature_dicts=convert_to_feature_dicts(trn_tweets,True,1)\n",
    "#proprocess development data\n",
    "dev_tweets,dev_labels=preprocess_file('dev.json')\n",
    "# convert development data into feature dictionaries\n",
    "dev_feature_dicts=convert_to_feature_dicts(dev_tweets,True,0)\n",
    "\n",
    "#convert the data to the sparse representation\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "def prepare_data(trn_feature_dicts,dev_feature_dicts):\n",
    "    vectorizer = DictVectorizer()\n",
    "    trn_feature_dicts = vectorizer.fit_transform(trn_feature_dicts)\n",
    "    dev_feature_dicts = vectorizer.transform(dev_feature_dicts)\n",
    "    return trn_feature_dicts,dev_feature_dicts\n",
    "\n",
    "#convert training data and development data into sparse representation\n",
    "trn_feature_dicts,dev_feature_dicts=prepare_data(trn_feature_dicts,dev_feature_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e72f22d1037544244612b2150d94b99bfbf7eb86a789835e278d779f"
   },
   "source": [
    "<b>Instructions</b>: Now, tune a decision tree classifier using accuracy in the development set as the evaluation metric. For this, you need to consider at least 2 parameters of the model likely to influence performance and which make sense in this context; you should read the documentation for the classifier on sci-kit learn website to learn what these parameters are. For any binary or categorical parameters, you should just consider all options. For numerical values, you should start by keep other settings on default and just randomly try a wide range, looking for values above which there is a steep drop-off in performance, or, alternatively, no effect on performance at all (you don't need to show this process in the notebook).  Remember that some parameters should be tested on a logarithmic scale. Once you're fairly confident of a good range for the parameter, divide it up into at least 5 steps (but no more than 10), and carry out a grid search, which is to say an exhaustive exploration of all parameter options within the limits you've set (this should be included in the notebook). Identify the best parameter values, and discuss the influence of the parameters on performance in the development set. Do you think some values of the parameters are resulting in overfitting?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "signature": "97ad269db9c0e5026aeb2cadf02f4a0f27a0a2b85df3bf4c16720891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.458720612356\n",
      "0.454346637507\n",
      "0.451612903226\n",
      "0.446145434664\n",
      "0.449425915801\n",
      "0.45106615637\n",
      "0.437397484964\n",
      "0.457080371788\n",
      "0.454893384363\n",
      "0.43083652269\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncpIyM7PC+v8lofq5A2QZ8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1100a0610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "'''\n",
    "test returns the accuracy of the classifier trained on the training data\n",
    "Input:\n",
    "clf: the classifier to be used\n",
    "training_data: the training data\n",
    "training_classifications: the labels of training data\n",
    "test_data: the data used to test the accuracy of the classifier trained on training data\n",
    "test_classifications: the labels of test data\n",
    "Return: accuracy\n",
    "'''\n",
    "def test(clf,training_data,training_classifications,test_data,test_classifications):\n",
    "\n",
    "    clf.fit(training_data,training_classifications)\n",
    "    predictions = clf.predict(test_data)\n",
    "    accuracy = accuracy_score(test_classifications,predictions)\n",
    "    return accuracy\n",
    "\n",
    "'''\n",
    "plot_graph plots the accuracy results of decision tree classifier\n",
    "Input: \n",
    "accuracy_list is used to store the accuracies\n",
    "range_list indicates the list of numbers of min_samples_split, which is parameter for decision\n",
    "tree classifier\n",
    "'''\n",
    "def plot_graph(accuracy_list,category_list,range_list):\n",
    "    n_groups = 5\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.4\n",
    "    rects1 = plt.bar(index, accuracy_list[0:5], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='b',\n",
    "                     label=category_list[0])\n",
    "    rects2 = plt.bar(index + bar_width, accuracy_list[5:10], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='r',\n",
    "                     label=category_list[1])\n",
    "\n",
    "    plt.xlabel('Classifier parameters')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy using different classifier parameters')\n",
    "    plt.xticks(index + bar_width, range_list)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "Train the classifier with different parameter settings. In this assignment,\n",
    "the parameter splitter(The strategy used to choose the split at each node. \n",
    "Supported strategies are “best” to choose the best split and “random” to \n",
    "choose the best random split.) and min_samples_split(The minimum number of \n",
    "samples required to split an internal node.) are examined. Splitter is set to\n",
    "'best' and 'random', while min_samples_split is set to 2,4,6,8,10\n",
    "'''\n",
    "\n",
    "accuracy_list=[]\n",
    "for split_method in ['best','random']:\n",
    "    for num_split in range(2,12,2):\n",
    "        clf1=DecisionTreeClassifier(splitter=split_method,min_samples_split=num_split)\n",
    "\n",
    "        acc= test(clf1,trn_feature_dicts,trn_labels,dev_feature_dicts,dev_labels)\n",
    "        print acc\n",
    "        accuracy_list.append(acc)\n",
    "        \n",
    "plot_graph(accuracy_list,['best','random'],range(2,12,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>\n",
    "From the above graph, we can see that both the parameters I chose do not have much effect on the final prediction accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "3586fbaaae10742a1f14cf65912fd539eb723b13e25d419ea1f91f49"
   },
   "source": [
    "<b>Instructions</b>: Carry out the same tuning process with the logistic regression classifier. Compare the performance of the two classifiers to each other, and to the most common class baseline. How are the classifiers doing? Is this a challenging task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "signature": "73eed72d2b12d09b05c29be321905bf8eaad8b8c009e3586dca40162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.501913613997\n",
      "0.501366867141\n",
      "0.501366867141\n",
      "0.501366867141\n",
      "0.501366867141\n",
      "0.506834335703\n",
      "0.504647348278\n",
      "0.504647348278\n",
      "0.504647348278\n",
      "0.504647348278\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1398fa950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "'''\n",
    "Build logistic regression model using different parameter settings, i.e. \n",
    "max_iter(Maximum number of iterations taken for the solvers to converge)\n",
    "and penalty(str, ‘l1’ or ‘l2’,Used to specify the norm used in the penalization).\n",
    "For this assignment, penalty is set to 'l1' and 'l2', while max_iter is set to 10,100,\n",
    "1000,10000,100000\n",
    "'''\n",
    "accuracy_list2=[]\n",
    "for penalty in ['l1','l2']:\n",
    "    for max_iter in range(1,6):\n",
    "        clf2=LogisticRegression(penalty=penalty,max_iter=10**max_iter)\n",
    "        acc= test(clf2,trn_feature_dicts,trn_labels,dev_feature_dicts,dev_labels)\n",
    "        print acc\n",
    "        accuracy_list2.append(acc)\n",
    "# plot the bar chart\n",
    "plot_graph(accuracy_list2,['l1','l2'],range(1,6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b> \n",
    "From the above graph, the parameters have no influence on the final result at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "88bc514c88161b16a2531906d60a0851ce0d940169b86ba3d7414918"
   },
   "source": [
    "<b>Instructions</b>: The next task is a slight detour to test your understanding of the logistic regression classifier: you are going to build your own classifier based on the trained model from sci-kit learn. In particular, you should fill in the MyLogisticRegression class started below which is initialized using the feature weights (coefficients) and constants (intercepts) and list of labels (classes) from the sci-kit learn classifier (see the \"Attributes\" in the documentation for the Logistic Regression classifier), and which mimics the predict and predict_proba methods from the sci-kit learn classifier object. You should confirm that your solution works by using it in the task at hand: take the classifier defined below, train it on the training data, then create an instance of MyLogisticRegression, and show that your classifier has the same output as the scikit-learn classifier for both predict and predict_proba for 5 samples from the development set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "signature": "81a3035ebbbd8625ede32260be722cf4ca25d8df6d80fab8578017fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  1 -1  1]\n",
      "[ 0  0  1 -1  1]\n",
      "[[ 0.06249832  0.92153973  0.01596195]\n",
      " [ 0.00207452  0.92469591  0.07322956]\n",
      " [ 0.38730555  0.18566726  0.42702719]\n",
      " [ 0.63109064  0.28846577  0.08044358]\n",
      " [ 0.04122416  0.35755894  0.6012169 ]]\n",
      "[[ 0.06249832  0.92153973  0.01596195]\n",
      " [ 0.00207452  0.92469591  0.07322956]\n",
      " [ 0.38730555  0.18566726  0.42702719]\n",
      " [ 0.63109064  0.28846577  0.08044358]\n",
      " [ 0.04122416  0.35755894  0.6012169 ]]\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "The logistic regression class implemented by myself. \n",
    "It uses the calculated coefficents and intercepts to predict the probability \n",
    "for different classes, or give the label of the class with highest probability \n",
    "score.\n",
    "'''\n",
    "class MyLogisticRegression:\n",
    "    '''\n",
    "    initialize the class with calculated coefficients, intercepts, and labels of the training \n",
    "    data\n",
    "    '''\n",
    "    def __init__(self, weights, constants, labels):\n",
    "        self.weights=weights\n",
    "        self.constants=constants\n",
    "        self.labels=labels\n",
    "    \n",
    "    '''\n",
    "    predict_proba takes the training data(which is already converted to a sparse matrix) as input,\n",
    "    do the calculations and return an array of probabilities, where each row corresponds to the\n",
    "    probabilities for all the classes for a specific tweet.\n",
    "    '''\n",
    "    def predict_proba(self,X):\n",
    "        prob=[]\n",
    "        X=X.toarray()\n",
    "        for index,x in enumerate(X):\n",
    "            numerator=[]\n",
    "            i=0\n",
    "            for weight in self.weights:\n",
    "                numerator.append(np.exp(np.dot(x, weight)+self.constants[i]))\n",
    "                i+=1\n",
    "            denominator=sum(numerator)\n",
    "            prob.append([])\n",
    "\n",
    "            for nume in numerator:\n",
    "                prob[index].append(nume/denominator)\n",
    "        return np.asarray(prob)\n",
    "    \n",
    "    '''\n",
    "    predict method takes the training data(which is already converted to a sparse matrix) as \n",
    "    input and returns the class label with the highest probability.\n",
    "    '''\n",
    "\n",
    "    def predict(self,X):\n",
    "        label_list=[]\n",
    "        X=X.toarray()\n",
    "        for index,x in enumerate(X):\n",
    "            numerator=[]\n",
    "            i=0\n",
    "            for weight in self.weights:\n",
    "                numerator.append(np.exp(np.dot(x, weight)+self.constants[i]))\n",
    "                i+=1\n",
    "            denominator=sum(numerator)\n",
    "            best_prob=-9e99\n",
    "            best_label=None\n",
    "            for index2,nume in enumerate(numerator):\n",
    "                class_prob=nume/denominator\n",
    "                if class_prob > best_prob:\n",
    "                    best_prob=class_prob\n",
    "                    best_label=self.labels[index2]\n",
    "            label_list.append(best_label)\n",
    "        return np.asarray(label_list)\n",
    "\n",
    "# Train a classifier using the training data\n",
    "clf3 = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "clf3.fit(trn_feature_dicts,trn_labels)\n",
    "#Get the classifier built by our own\n",
    "myLR=MyLogisticRegression(clf3.coef_, clf3.intercept_, clf3.classes_)\n",
    "\n",
    "#To compare my results with the scikit learn classifier\n",
    "print myLR.predict(dev_feature_dicts)[0:5]\n",
    "print clf3.predict(dev_feature_dicts)[0:5]\n",
    "\n",
    "print myLR.predict_proba(dev_feature_dicts)[0:5]\n",
    "print clf3.predict_proba(dev_feature_dicts)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Analysis</b>\n",
    "From the above result, we can see that the two classifiers give the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "7eb4249c43c16b198796115b2b1977a30e1f32cf0e8efde44391938f"
   },
   "source": [
    "## Polarity Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "222499934251174c01259d47f9aa0b77fc082c539f3ec8fc74293fb6"
   },
   "source": [
    "<b>Instructions</b>: Next we will try integrating information from sources beyond the training set, in the form of polarity lexicons. The main focus of this section is producing and evaluating 3 automatically-built polarity lexicons. The first of these lexicons is SentiWordNet, which is <a href=\"http://www.nltk.org/howto/sentiwordnet.html\"> accessible through NLTK</a>. SentiWordNet has precalculated scores for positive, negative, and neutral sentiment for some of the words in WordNet, but, like WordNet, it is arranged in synsets; building a WSD system to handle this is beyond the scope of this assignment, instead you should take the most common polarity across its senses (neutral if there is a tie). Do this by iterating through all the synsets in WordNet (which may take a little while, the code snippet below has a counter to show your progress), and then create two lists, one of positive words, one of negative words. Show 5 examples of each of the positive and negative words, and comment on their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "signature": "89220443510ad7c018f80daf991ee34cba839e7b5eaaff0adf64ebf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. positive list negative list\n",
      "[u'gossamer', u'Mighty_Mouse', u'magnetic', u'munificence', u'idealised'] [u'funereal', u'unscientific', u'foul', u'aggression', u'ill_nature']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_polarity_type(synset_name):\n",
    "    swn_synset =  swn.senti_synset(synset_name)\n",
    "    if not swn_synset:\n",
    "        return None\n",
    "    elif swn_synset.pos_score() > swn_synset.neg_score() and swn_synset.pos_score() > swn_synset.obj_score():\n",
    "        return 1\n",
    "    elif swn_synset.neg_score() > swn_synset.pos_score() and swn_synset.neg_score() > swn_synset.obj_score():\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "positive_list1=[]\n",
    "negative_list1=[]\n",
    "\n",
    "#count = 0\n",
    "for synset in wn.all_synsets():\n",
    "    #count += 1\n",
    "    #if count % 1000 == 0:\n",
    "        #print count\n",
    "    # count synset polarity for each lemma\n",
    "    name=synset.name()\n",
    "\n",
    "    polarity_type=get_polarity_type(name)\n",
    "    if polarity_type is not None:\n",
    "        if polarity_type ==1:\n",
    "            # if the polarity type of the synset name is 1\n",
    "            # add the lemma to the positive list\n",
    "            positive_list1+=synset.lemma_names()\n",
    "        elif polarity_type== -1:\n",
    "            # if the polarity type of the synset name is -1\n",
    "            # add the lemma to the negative list\n",
    "            negative_list1+=synset.lemma_names()\n",
    "\n",
    "positive_list1=list(set(positive_list1))\n",
    "negative_list1=list(set(negative_list1))\n",
    "print '1. positive list negative list'\n",
    "print positive_list1[0:5],negative_list1[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>\n",
    "From the above positive list and negative list, we can see that all the words in the positive list can be related to something positive. If they are adjectives, they can be used to discribe something good, or if they are nouns, they are something with good quality or represent a good characteristic of a person. The negative word list is also of good quality, all of the 5 words can be related to something bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9ce3937489fc060ba10b096aae24b2b953c9fd104f244c92c97c82e0"
   },
   "source": [
    "<b>Instructions</b>: The second lexicon will be built using the word2vec (CBOW) vectors included in NLTK. For this, you will need a small set of positive and negative seed terms, which are given to you below. Calculate cosine similarity between vectors of the seeds terms and each of the words for which you have vectors (if you use Gensim, you can iterate over model.vocab), flip the sign for the negative seeds, and then average to get a score. Use this score to produce a list of positive and negative words; you should include a threshold of ±0.03 for words to be considered positive or negative. Again, show 5 examples of each of the positive and negative words, and comment on their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "signature": "9931c8c4464e5da7da0b9b5a2fea8e9cd12a5a5fcc8935a40c27ee4e"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named gensim",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e3113fbe226f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpositive_seeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"good\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"nice\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"excellent\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"positive\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"fortunate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"correct\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"superior\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"great\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnegative_seeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"bad\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"nasty\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"poor\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"negative\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"unfortunate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wrong\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"inferior\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"awful\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named gensim"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.data import find\n",
    "\n",
    "positive_seeds = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\",\"great\"]\n",
    "negative_seeds = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\",\"awful\"]\n",
    "'''\n",
    "second_lexicon takes the positive_seeds and negative_seeds as input, use the sample model from \n",
    "'models/word2vec_sample/pruned.word2vec.txt'. For every word in the model.vocab, calculate the \n",
    "model similarity of it and the word from positive or negative seed. Flip the sign for negative \n",
    "seed and sum them up, then take the average. If the score is more than 0.03, then the word is\n",
    "added to the positive list, if the score is less than 0.03, then the word is added to the negative\n",
    "list.\n",
    "'''\n",
    "def second_lexicon(positive_seeds,negative_seeds):\n",
    "\n",
    "    word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(word2vec_sample, binary=False)\n",
    "    positive_list=[]\n",
    "    negative_list=[]\n",
    "\n",
    "    for aword in model.vocab:\n",
    "        score=0\n",
    "        for pseed in positive_seeds:\n",
    "            score+=model.similarity(aword, pseed)\n",
    "        for nseed in negative_seeds:\n",
    "            score-=model.similarity(aword,nseed)\n",
    "\n",
    "        score=score/16.0\n",
    "        if score>0.03:\n",
    "            positive_list.append(aword)\n",
    "        elif score<-0.03:\n",
    "            negative_list.append(aword)\n",
    "\n",
    "    return positive_list,negative_list\n",
    "\n",
    "positive_list2, negative_list2=second_lexicon(positive_seeds, negative_seeds)\n",
    "print ' 2. positive list negative list'\n",
    "print positive_list2[0:5], negative_list2[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "0bae4e130bdd5a1c767517625a7f0b686e7fb27448d03a3f41358314"
   },
   "source": [
    "<b>Instructions</b>: The third lexicon will be built by calculating PPMI with the seed terms. For this, use the Brown corpus included in NLTK, with co-occurrence defined as <em>binary</em> text co-occurrence (that is, multiple co-occurrences in the same text are not counted); importantly, your solution should <em>not</em> calculate the entire co-occurrence matrix, since you only care about relative co-occurrence with the seeds. As above, average the resulting similarity scores after switching the sign for the negative seeds and use them to produce a list of positive and negative words, and check 5 of each. For PPMI, use a threshold of  ±0.3 for deciding if a word is neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "signature": "6dd1b963e2f08f3169f207703e2558f8f3b28e42f002125372f43d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. positive list negative list\n",
      "[u'francesca', u'comically', u'spidery', u'ultra-violet', u'non-violent'] [u'bilharziasis', u'pigment', u'wooden', u'deferments', u'blot-appearance']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import math\n",
    "positive_seeds = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\",\"great\"]\n",
    "negative_seeds = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\",\"awful\"]\n",
    "\n",
    "'''\n",
    "get_BOW takes a list of strings as input, return a dictionary with words as keys and the number\n",
    "of times that word occurs as values.\n",
    "'''\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word.lower()] = BOW.get(word.lower(),0) + 1\n",
    "    return BOW\n",
    "'''\n",
    "third_lexicon takes the positive seeds and negative seeds as input, and returns lists of \n",
    "positive words and negative words.\n",
    "First a dictionary all_dic is created with all the words in brown.words() as entries. Each entry is another\n",
    "dictionary with positive seeds, negative seeds and 'word_count' as keys. The values of positive\n",
    "seed keys represent the the co-occurence times of that specific word with the positive seed, and\n",
    "likewise for negative seeds. The value in 'word_count' represents the number of times that word \n",
    "occur throughout the document. Another dicitionary constructed is seed_total_dic, it stores the \n",
    "number of times each seed occurs in the document. \n",
    "For the co-occurence calculation, multiple co-occurences in the same text is calculated as one.\n",
    "\n",
    "The 0 and negative PMI scores are discarded, and the signs of PMI scores with negative seeds are \n",
    "flipped. The sum is calculated and then divided by 16 to get the average. This figure is then \n",
    "compared to +0.3,-0.3 to add words to positive list and negative list. \n",
    "'''\n",
    "def third_lexicon(positive_seeds,negative_seeds):\n",
    "    positive_list=[]\n",
    "    negative_list=[]\n",
    "\n",
    "    all_dic={}\n",
    "    seed_total_dic={}\n",
    "    for fileid in brown.fileids():\n",
    "        bow=get_BOW(brown.words(fileid))\n",
    "        for aword in bow:\n",
    "            all_dic[aword]=all_dic.get(aword,{})\n",
    "            all_dic[aword]['word_count']=all_dic[aword].get('word_count',0)+1\n",
    "            for pseed in positive_seeds:\n",
    "                if pseed in bow:\n",
    "                    all_dic[aword][pseed]=all_dic[aword].get(pseed,0)+1\n",
    "            for nseed in negative_seeds:\n",
    "                if nseed in bow:\n",
    "                    all_dic[aword][nseed]=all_dic[aword].get(nseed,0)+1\n",
    "\n",
    "        for pseed in positive_seeds:\n",
    "            if pseed in bow:\n",
    "                seed_total_dic[pseed]=seed_total_dic.get(pseed,0)+1\n",
    "        for nseed in negative_seeds:\n",
    "            if nseed in bow:\n",
    "                seed_total_dic[nseed]=seed_total_dic.get(nseed,0)+1\n",
    "\n",
    "    total_count=float(len(brown.fileids()))\n",
    "\n",
    "    for aword in all_dic:\n",
    "        score=0\n",
    "        for pseed in positive_seeds:\n",
    "            if all_dic[aword].get(pseed) != None:\n",
    "                a_score=math.log((all_dic[aword][pseed]/total_count)/((all_dic[aword]['word_count']/total_count)*(seed_total_dic[pseed]/total_count)), 2)\n",
    "                if a_score>0:\n",
    "                    score+=a_score\n",
    "\n",
    "        for nseed in negative_seeds:\n",
    "            if all_dic[aword].get(nseed) != None:\n",
    "                a_score=math.log((all_dic[aword][nseed]/total_count)/((all_dic[aword]['word_count']/total_count)*(seed_total_dic[nseed]/total_count)), 2)\n",
    "                if a_score>0:\n",
    "                    score-=a_score\n",
    "\n",
    "\n",
    "        score=score/16.0\n",
    "\n",
    "\n",
    "        if score>0.3:\n",
    "            positive_list.append(aword)\n",
    "        elif score<-0.3:\n",
    "            negative_list.append(aword)\n",
    "\n",
    "    return positive_list,negative_list\n",
    "\n",
    "positive_list3, negative_list3=third_lexicon(positive_seeds, negative_seeds)\n",
    "print '3. positive list negative list'\n",
    "print positive_list3[0:5], negative_list3[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>\n",
    "The resulted list does not seem to be of good quality. Francesca is a name and ultra-violet is an object, they both should be neutral rather than positive. Comically and spidery are more likely to be neutral words as well. As for the negative list, pigment and wooden do not seem negative. \n",
    "This is mainly due to the fact that the PPMI scores are calculated on a text level instead of a sentence level. The words in the same text are less correlated to each other than the words in the same sentence, thus resulting in a list of bad quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "df2905143fcc1c2614933587329996421e16ea24a28c95f81d8c1a39"
   },
   "source": [
    "<b>Instructions</b>: Now you will test these automatically-produced lexicons against a manually-annotated set. There is a manually-built lexicon (the Hu and Liu lexicon) which is included with NLTK. It has a list of positive and negative words, which are accessed as below. First, investigate what percentage of the words in the manual lexicon are in each of the automatic lexicons, and then, only for those words which overlap and which are <em>not</em> in the seed set, evaluate the accuracy of with each of the automatic lexicons. Discuss the results, mentioning why you think the lexicon which won out did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "signature": "a21792bdaddd795fa32b1dc88bfdd995cd5849f8a0e06a09071b09e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicon 1 positive: 0.272183449651\n",
      "lexicon 1 negative: 0.288939995819\n",
      "lexicon 3 positive: 0.0438683948156\n",
      "lexicon 3 negative: 0.0449508676563\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "'''\n",
    "calculate_percentage takes two lists as input, return the percentage of the overlapping words\n",
    "in the first list.\n",
    "'''\n",
    "def calculate_percentage(manual,automatic):\n",
    "    #automatic=set(automatic)\n",
    "    count=0\n",
    "    for word in manual:\n",
    "        if word in automatic:\n",
    "            count+=1\n",
    "    return float(count)/len(manual)\n",
    "\n",
    "\n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()\n",
    "\n",
    "print 'lexicon 1 positive:',calculate_percentage(positive_words,positive_list1)\n",
    "print 'lexicon 1 negative:',calculate_percentage(negative_words,negative_list1)\n",
    "'''\n",
    "print 'lexicon 2 positive:',calculate_percentage(positive_words, positive_list2)\n",
    "print 'lexicon 2 negative:',calculate_percentage(negative_words, negative_list2)\n",
    "'''\n",
    "print 'lexicon 3 positive:',calculate_percentage(positive_words, positive_list3)\n",
    "print 'lexicon 3 negative:',calculate_percentage(negative_words, negative_list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "4c379c7fd57895c160f0dce018da0db65fceef7f344a669a01c9cabe"
   },
   "source": [
    "<b>Instructions</b>: Now you will use the lexicons (both manual and automatic) for the main classification problem. Create a function which calculates a polarity score for a sentence based on a given lexicon (i.e. counting positive and negative words that appear in the tweet, and then returning +1 if there are more positive words, -1 if there are more negative words, and 0 otherwise). Then, use this to compare the results of the different lexicons (please convert them to sets!) on the task in the development set, i.e. the accuracy relative to the human-annotated labels. Do the results reflect the quality of the lexicon as indicated by the earlier analysis? How does it compare to the logistic regression classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "signature": "ebef05138e316018347a5747092dc11105a230d38770462954d39a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of lexicon\n",
      "Hu and Liu lexicon: 0.453799890651\n",
      "lexicon 1: 0.41498086386\n",
      "lexicon 3: 0.363039912521\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "my_polarity takes a list and two sets as input, the list represents a single tweet and pset is \n",
    "positive word set and nset the negative word set. If there are more words in pset, return 1. If\n",
    "there are more words in nset, return -1.\n",
    "'''\n",
    "def my_polarity(tweet,pset,nset,):\n",
    "    score=0\n",
    "    for word in tweet:\n",
    "        if word in pset:\n",
    "            score+=1\n",
    "        elif word in nset:\n",
    "            score-=1\n",
    "    if score>0:\n",
    "        return 1\n",
    "    elif score<0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "'''\n",
    "accuracy_of_lexicon takes a tweets list, a labels list and a positive list, a negative list. It \n",
    "calculates the accuracy of the input lexicon.\n",
    "'''\n",
    "def accuracy_of_lexicon(tweets,labels,plist,nlist):\n",
    "    count=0\n",
    "    pset=set(plist)\n",
    "    nset=set(nlist)\n",
    "\n",
    "    for tweet,label in zip(tweets,labels):\n",
    "        if label==my_polarity(tweet,pset,nset):\n",
    "            count+=1\n",
    "\n",
    "    return float(count)/len(labels)\n",
    "\n",
    "print 'accuracy of lexicon'\n",
    "print 'Hu and Liu lexicon:',accuracy_of_lexicon(dev_tweets,dev_labels,positive_words,negative_words)\n",
    "print 'lexicon 1:',accuracy_of_lexicon(dev_tweets,dev_labels,positive_list1,negative_list1)\n",
    "#print 'lexicon 2:',accuracy_of_lexicon(dev_tweets,dev_labels,positive_list2,negative_list2)\n",
    "print 'lexicon 3:',accuracy_of_lexicon(dev_tweets,dev_labels,positive_list3,negative_list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "a5b0e40cf431f6ecb3e0e18d6b2ab9213ddcd1198d1cb6bfa0642dbc"
   },
   "source": [
    "<b>Instructions</b>: Now you should investigate the effect of adding the polarity score (or scores) as a feature in your statistical classifier. You should create a new version of your convert_to_feature_dict function (with a different name) to include the extra feature (or features), do not modify the code in that earlier section directly. Retrain your best logistic regression classifier from the early tuning, test on the development set. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "f9bf187f20f9c509f47c8b184038fa011048d11feca82ca91b40a55f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "5ffc558321e254f8a6893e5affb98867a893b1ffbab8a813756b6e71"
   },
   "source": [
    "## Error analysis and improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e140733186799a3e75492cb90d87bc53e639ef62db47c93afffb4cf2"
   },
   "source": [
    "<b>Instructions</b>: Using your best logistic regression classifier so far, first write a function to identify errors your classifier is making where the probability of the predicted class and the actual class are fairly close (less than 0.2); you're looking for cases which you have a good chance of getting right with a small improvement. For this, do an 80/20 split of  the training dataset (that is, train on 80% of the data, test on 20%); do not look at examples from the development set or the test set. You should print out the tweet, the correct class, the predicted class, and the probabilities. Don't print all the errors, just use random.sample to select 30 from the full set. Look for general patterns in the errors, and propose a reasonable improvement to your classifier that you think might help with a problem that you are seeing. It could involve, for instance, better preprocessing, the addition of new features, some kind of feature selection, better lexicons or better use of the lexicons, or even a post-processing step. It should not require additional data, unless it involves a small set of words that you can hardcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "signature": "d93d0f3cdc8a2acbb253fe0c200fe8421d8d15a6346c8b239dedafcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'text': u\"Feel sorry for anyone going to the Justin Beiber concert tomorrow, they're missing out on one of the best games of the night\", u'id': u'900927715', u'label': -1}\n",
      "[-1, 0, 0.3465201093080395, 0.45564566602955964]\n",
      "{u'text': u'OK SORRY SM Sta. Rosa, November 10! See you guys! =)', u'id': u'36909155', u'label': 1}\n",
      "[1, 0, 0.46021443655187311, 0.52350488517115523]\n",
      "{u'text': u'@Diana_Gomez90 Text me tomorrow if you get a chance! I watched TVD tonight', u'id': u'69449995', u'label': 0}\n",
      "[0, 1, 0.44989738399991602, 0.48471868115458638]\n",
      "{u'text': u\"lmao my last retweet, I was popped from 6th grade to sophomore year, Jr year I was doing something, and Sr year, you couldn't tell me shit\", u'id': u'231316097', u'label': 0}\n",
      "[0, 1, 0.32506130123989824, 0.38661421371182625]\n",
      "{u'text': u\"My dads all buying beds for the house in Wisconsin . Like bitch I haven't been there since 6th grade\", u'id': u'333651366', u'label': -1}\n",
      "[-1, 0, 0.39096563979416354, 0.50480352870853518]\n",
      "{u'text': u'I rate Lee Dickson but Danny Care has to be on the bench sat for the 2nd test 60 mins with tired legs he will be dangerous #rugby #saeng', u'id': u'408515870', u'label': -1}\n",
      "[-1, 0, 0.46746821501956359, 0.53062934532065709]\n",
      "{u'text': u'2nd night going to sleep with this girl on skype lol @xJNasty <3 http://t.co/Ic2AJf5G', u'id': u'457105331', u'label': 0}\n",
      "[0, 1, 0.39374860392449745, 0.58889781174782163]\n",
      "{u'text': u'The sit down with Peyton was gr8..He opens up alot. Tune in Sunday on the Nfl today on CBS.', u'id': u'371539066', u'label': 1}\n",
      "[1, 0, 0.42310994749142028, 0.52319654690691253]\n",
      "{u'text': u\"@SpicyMamacita69 I hate those movies and Devil Inside, and I think it's called 4th kind. All those documentary type.\", u'id': u'349434644', u'label': -1}\n",
      "[-1, 0, 0.48099436023006031, 0.51129500709170472]\n",
      "{u'text': u'In our busy lives in Dubai could we just spare a moment of silence this Friday morning for the people who still wear crocs.', u'id': u'189813479', u'label': 0}\n",
      "[0, 1, 0.3893494444834803, 0.51750729380722049]\n",
      "{u'text': u\"PSN Tuesday: Assassin's Creed 3, Okami HD:  You can buy all the Assassin's Creed you can handle on PSN tod... http://t.co/rBMWrtpC _ _!)\", u'id': u'491149965', u'label': 0}\n",
      "[0, -1, 0.45772386629762707, 0.51705501171570689]\n",
      "{u'text': u\"@ZakAdamz decent that bruv n I'm looking the Espanyol game in jan the 5th or 6th ..\", u'id': u'85446794', u'label': 0}\n",
      "[0, 1, 0.2269147386728364, 0.40174633472410659]\n",
      "{u'text': u'@proudliberal63 really debating Florida. Need to look at a few more sources and color it tomorrow.', u'id': u'357856815', u'label': 0}\n",
      "[0, 1, 0.38220188719587528, 0.39151127632190347]\n",
      "{u'text': u\"@zminnis4 I better see you at Jason's on Friday or Saturday\", u'id': u'292570360', u'label': 0}\n",
      "[0, 1, 0.39651382998054929, 0.55399519958980314]\n",
      "{u'text': u'If we grabbed those people in the sun life stadium n place em in the orange bowl te stadium would look pack n louder', u'id': u'237060289', u'label': 0}\n",
      "[0, 1, 0.42662614149250017, 0.53244948647257562]\n",
      "{u'text': u'2nd time to New Creation Church to see Joseph Prince for the 1st time. http://t.co/Ch3y5HKG', u'id': u'212136615', u'label': 0}\n",
      "[0, 1, 0.45227172464758297, 0.50837173541477199]\n",
      "{u'text': u'@HassanMBD haha i havent worn shalwar kameez for about 10 years. You went Bolton darul uloom? MashaAllah. May I ask.... Hifz? Aalim class?', u'id': u'221489440', u'label': 0}\n",
      "[0, -1, 0.39970203626774331, 0.53859421796276963]\n",
      "{u'text': u\"I really don't want to go to ISS tomorrow... -_-\", u'id': u'353806802', u'label': -1}\n",
      "[-1, 0, 0.43142583043725202, 0.45301629807825439]\n",
      "{u'text': u'It looks like the Curtis Painter era of Colts may begin tonight!', u'id': u'929', u'label': 1}\n",
      "[1, 0, 0.42489394446622003, 0.50205161438381729]\n",
      "{u'text': u\"@LetsGoTribe Must. Get. Innings.   We must pile on and decimate their bullpen tonight! Because with Verlander, they won't need it tomorrow.\", u'id': u'133132341', u'label': 0}\n",
      "[0, 1, 0.24294039604044668, 0.40637617708165596]\n",
      "{u'text': u\"Who else thinks that the 'Little Things' video that premieres November 2nd is gonna beat Justin Biebers record?\", u'id': u'842580930', u'label': 0}\n",
      "[0, 1, 0.42915876808972742, 0.50407669356297247]\n",
      "{u'text': u\"it's a sunday, i think in the spirit of douglas adams and feeling like shit i will have a second bath\", u'id': u'14314544', u'label': -1}\n",
      "[-1, 0, 0.41785891140780729, 0.52639074113625917]\n",
      "{u'text': u'I just watched part of #DWTS and cowboy Troy was on there singing I play chicken with a train. brings back 8th grade memories', u'id': u'261836654', u'label': 0}\n",
      "[0, -1, 0.37884061703440386, 0.38369114900846341]\n",
      "{u'text': u'February 13 is going to kill me @Maroon5 @adamlevine @jamesbvalentine thanks for skipping Indiana -.-', u'id': u'494417056', u'label': -1}\n",
      "[-1, 1, 0.28385381931617815, 0.39199498335866095]\n",
      "{u'text': u\"My dads all buying beds for the house in Wisconsin . Like bitch I haven't been there since 6th grade\", u'id': u'333651366', u'label': -1}\n",
      "[-1, 0, 0.39096563979416354, 0.50480352870853518]\n",
      "{u'text': u\"Watched the 1st episode of Being Human. @Bains_Prince, I'm hooked. Thought you'd like to know.\", u'id': u'80414873', u'label': 1}\n",
      "[1, 0, 0.40856674154856409, 0.42811380535442356]\n",
      "{u'text': u\"In case you were unawares, Dwight Howard is absolutely going HAM in LA right now. Like WOW. It's only the 3rd Q.\", u'id': u'304453994', u'label': 1}\n",
      "[1, 0, 0.20830008111523948, 0.39863783231085032]\n",
      "{u'text': u\"lmao my last retweet, I was popped from 6th grade to sophomore year, Jr year I was doing something, and Sr year, you couldn't tell me shit\", u'id': u'231316097', u'label': 0}\n",
      "[0, 1, 0.32506130123989824, 0.38661421371182625]\n",
      "{u'text': u'Why are the regular Thursday night shows not on tonight? I was really looking forward to Parks and Rec. :(', u'id': u'46763476', u'label': -1}\n",
      "[-1, 0, 0.29748824949094133, 0.36765199239698326]\n",
      "{u'text': u'Gabe Carimi called for his 2nd holding and 3rd in 2 games. But got lucky with offsetting calls. Its his first time facing DE Cliff Avril.', u'id': u'140109759', u'label': 1}\n",
      "[1, 0, 0.35981981503859117, 0.45065497886776834]\n"
     ]
    }
   ],
   "source": [
    "# get the classifier with best performance\n",
    "clf_err= LogisticRegression()\n",
    "\n",
    "'''\n",
    "get_err_list takes the trn_feature_dicts(a sparse matrix), trn_labels, and clf_err(a logistic\n",
    "regression classifier) as input, and returns a list of mis-predicted tweets,its actual label, \n",
    "predicted label, the probability for its actual label, and the probability for its predicted \n",
    "label.\n",
    "\n",
    "The classifier is trained on 4/5 of the training data, and tested on the rest 1/5 data.\n",
    "'''\n",
    "def get_err_list(trn_feature_dicts,trn_labels,clf_err):\n",
    "    len_trn=trn_feature_dicts.shape[0]\n",
    "    clf_err.fit(trn_feature_dicts.toarray()[0:len_trn*4/5],trn_labels[0:len_trn*4/5])\n",
    "    predictions=clf_err.predict(trn_feature_dicts[len_trn*4/5:len_trn])\n",
    "    prediction_probs=clf_err.predict_proba(trn_feature_dicts[len_trn*4/5:len_trn])\n",
    "    i=0\n",
    "    err_list=[]\n",
    "    # pre is predicted label, label is the actual correct label\n",
    "    for pre,label in zip(predictions,trn_labels[len_trn*4/5:len_trn]):\n",
    "        if pre!=label:\n",
    "            probs = prediction_probs[i]\n",
    "            # the probability for the predicted label and the probability for the actual label\n",
    "            predict_prob=0\n",
    "            actual_prob=0\n",
    "            for index,class_label in enumerate(clf_err.classes_):\n",
    "                if class_label==label:\n",
    "                    actual_prob=probs[index]\n",
    "                elif class_label==pre:\n",
    "                    predict_prob=probs[index]\n",
    "            # If the two probabilities are fairly close\n",
    "\n",
    "            if predict_prob-actual_prob<0.2:\n",
    "                err_list.append([original_tweet[len_trn*4/5+i],label,pre,actual_prob,predict_prob])              \n",
    "        i+=1\n",
    "    return err_list\n",
    "\n",
    "err_list=get_err_list(trn_feature_dicts,trn_labels,clf_err)\n",
    "random_index=np.random.randint(len(err_list), size=30)\n",
    "for an_index in random_index:\n",
    "    print err_list[an_index][0]\n",
    "    print err_list[an_index][1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "03077b4b348fb16cab9f35475a99a16df22012d2107d4592d7b9a7d3"
   },
   "source": [
    "<b>Instructions</b>: Now implement that improvement, and then investigate its effect <em>in the development data</em>. Obviously, different improvements may involve different amounts of effort; if your improvement is fairly simple, we expect that you will do a more in-depth analysis, testing possible variations. You can also do multiple related improvements. Students who put extra effort into this may get few extra points that can offset any mistakes on other parts of the assignment, though we do not recommend you spend extra time on this before the other parts of the assignment are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "ab77e3b674fbb64ad43b2f03bdcc66cd9cffed98de849fd115a44072"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "2b0f4d9e8ab0fe8d28e48b8dba46d891745a82101b77551f31f52ff2"
   },
   "source": [
    "## Final testing and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "c56e9bb9790acf94081360a67b83dc79b7a5a96f159c21d2ccf2f9d3"
   },
   "source": [
    "<b>Instructions</b>: When the final test set has been released, you should start with your best classifier from your work up to this point, and do a final test of all major options, including at least one from each of the first four sections of the assignment (that is, at least one preprocessing option, at least one tuning parameter/classifier type, at least one lexicon, and your improvement), in this new dataset. You don't need to explore every possible combination (in fact, you shouldn't), but you should make a convincing case that you have probably found the best combination given the possibilities you have implemented. It's okay if you find discrepancies between the best classifier on the test set and development set, just be sure to mention them. In a final discussion (which should be at least 500 words), you should include at least one bar graph of accuracy across various options, and at least one table which reports precision, recall, and F-score for each label as well as the macroaveraged F-score (all figures and tables should be generated inline by your code, using matplotlib). Please conclude your discussion by discussing what you have learned, and mentioning any other ideas for improving performance of this system that you may have.\n",
    "\n",
    "Note that you may have to direct matplotlib to display the figures inline, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "signature": "8c26ed656bdee89c90809063e6a99a73f200067917ed00f4de13cb84"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
