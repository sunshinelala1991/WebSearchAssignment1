{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "fb64f22f4b9d6a57ea7261cf771e646e223fad9c85d255dda4a7efa4"
   },
   "source": [
    "# COMP90042 Assignment #1: Sentiment analysis for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "signature": "dbe5c8ee5ef597fc2680bec06e686a52a01edd44992269bd65c07d09"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-8166fd4864cd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-8166fd4864cd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Student Name: Yun Wang\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Student Name: Yun Wang\n",
    "Student ID: 672323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "5653cf33f236b021076096666f0eaca598a8b6ea818c0d2a6425a15b"
   },
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "eac8f7335062ea69220caf0a93873dbcb9121a0c0d8bafc349dbbfff"
   },
   "source": [
    "<b>Due date</b>: 5pm, Mon April 12\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this ipython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day, no late submissions after the first week\n",
    "\n",
    "<b>Marks</b>: 25% of mark for class\n",
    "\n",
    "<b>Overview</b>: For this project, you'll be building a 3-way polarity classification system for tweets, using a logistic regression classifier, BOW features, as well as polarity lexicons built from external sources. A key focus of this project is critical analysis and experimental evaluation, for which you will need to report on the relative merits of various options. \n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Sci-kit Learn, and Gemsim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python build-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. You are encouraged to use the iPython notebooks released for this class as well as other online documentation to guide your responses, but you should not copy directly from any source. The only other data you will need is three sets of tagged tweets, the first two of which (training and dev) were released at the same time as this notebook, and a third set (test) which will be made available about a week before the assignment is due, see Final Testing below. This data comes from the recent SemEval 2016 shared task. Do not distribute this data indiscriminately (i.e. put it on a public website), you should use it only for this assignment, and delete it afterwards. The corpus is comprised of unfiltered text from the web, and may include offensive or objectionable material. This reflects the general composition of the web and the general challenges present in web based text analysis. The University of Melbourne takes no responsibility for opinions expressed in the corpus, nor takes any responsibility for offence caused by these documents.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time (less than 10 minutes on a lab desktop), and you must follow all instructions provided below, including specific implementation requirements. You will be marked not only on the correctness of your methods, but also on your explanation and analysis. Please do not change any of instruction text in the notebook. Where applicable, leave the output cells in the code, particularly when you are commenting on that output. You should add your answers and code by inserting a markdown cell between every major function or other block of code explaining its purpose or anywhere a result needs to be discussed (see the class notebooks for examples). Note that even if you do something wrong, you might get partial credit if you explain it enough that we can follow your reasoning, whereas a fully correct assignment with no text commentary will not receive a passing score. You will not be marked directly on the performance of your final classifier, but each of the steps you take to build it should be reasonable and well-defended.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this project, and we encourage you to discuss it in general terms with other students. However, it is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "55678c4756c3c6d4aa908b80503b61a10c423f53cd814b52ad443b54"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "9c1902745267a3e3d99862e5ec6431ab9f901f75baa61ef5a024a968"
   },
   "source": [
    "<b>Instructions</b>: Your first task is to carry out preprocessing on the tweets. Use the code below as a starter. Each line of the input files is a json including the tweet and the label (and the tweet id), this code just loads them into a list without any preprocessing. Note that for the labels, 1 = positive, 0 = neutral, -1 = negative. Here is a list of things your preprocessing code must do:\n",
    "\n",
    "<ul>\n",
    "<li>Segment into sentences: Use NLTK punkt sentence segmenter</li>\n",
    "<li>Tokenize sentences: Use the NLTK regex WordPunct tokenizer</li>\n",
    "<li>Lowercase all words</li>\n",
    "<li>Remove Twitter usernames: Usernames on twitter begin with @</li>\n",
    "<li>Remove URLs: URLs start with http</li> \n",
    "<li>Remove any hashtags from their original location in the tweet, tokenize them, and add them as a separate sentences with the hash tag removed: for tokenization, use capitalized letters when they occur (e.g. #RefugeesWelcome -> Refugees Welcome), or when there is no capitalization (#refugeeswelcome -> refugees welcome) use the MaxMatch algorithm and the list of English words included in NLTK (nltk.corpus.words.words()). Two notes about the English word list: 1. you should convert it to a python set before you use it (sets are hashed, so you get much quicker lookup) 2. It contains only base forms, so you will need to lemmatize words before you look them up.</li>\n",
    "</ul>\n",
    "\n",
    "You can do these in almost any order you like, but it may be useful to do the main segmentation/tokenization last (or almost last), since for the other tasks it is easier to deal with the raw string rather than a list of tokens. The use of regular expressions is recommended, but not required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "signature": "d206ed2a6dd099cad03569db61a8b4b5de7e6ec1c4590e340a588a93"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "word_set=set(nltk.corpus.words.words())\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "# the number of tweets required to print\n",
    "num_tweet_print=1\n",
    "\n",
    "'''\n",
    "Preprocess a single tweet.\n",
    "The method takes a single tweet as input, remove all the '@'s and urls at first, \n",
    "then find all the hashtags in this tweet and store them into a list. Then the hashtags \n",
    "are tokenized based on whether there is capticalization. If yes, it is splitted using the capital \n",
    "letters, if not, it is splitted using maxMatch algorithm-- lemmatize the substring first, then \n",
    "look it up in the word_set(nltk.corpus.words.words()). After this, all the hashtags are removed \n",
    "from tweet and the tweet is tokenized into sentences then words. Finally, the list of words inside\n",
    "the hashtag is added to the end of the list of words from tweet and returned.\n",
    "'''\n",
    "def preprocess(ori_tweet):\n",
    "    global num_tweet_print\n",
    "    # get rid of all the \"@\"s and urls\n",
    "    tweet = re.sub(\"@[^ ]+\", \"\", ori_tweet).strip()\n",
    "    tweet=re.sub(\"http[^ ]+\",\"\",tweet).strip()\n",
    "    # Find all the hashtags and put them in a list called hashtag\n",
    "    hashtag = re.findall('#[^ ]*', tweet)\n",
    "    tag_list=[]\n",
    "    if hashtag:\n",
    "        for tag in hashtag:\n",
    "            #remove the \"#\" from one hashtag\n",
    "            tag=re.sub('#','',tag)\n",
    "            # if contains capital letters\n",
    "            if re.search('.*[A-Z].*',tag):\n",
    "                tag_list=tag_list+re.findall('[A-Z][^A-Z]*',tag)\n",
    "\n",
    "            else:\n",
    "                 # if not contain capital letters use a maxmatch algorithm to find the words\n",
    "                i=0\n",
    "                while i<len(tag):\n",
    "                    for j in range(len(tag),i,-1):\n",
    "                        #lemmatize words first\n",
    "                        lemma=lemmatize(tag[i:j])\n",
    "                        if lemma in word_set:\n",
    "                            tag_list.append(tag[i:j])\n",
    "                            i=j-1\n",
    "                    i+=1\n",
    "    if tag_list:\n",
    "        for i in range(len(tag_list)):\n",
    "            tag_list[i]=tag_list[i].lower()\n",
    "    #remove all the hashtags\n",
    "    tweet = re.sub('#[^ ]*','',tweet).lower()\n",
    "    #split tweet into sentences\n",
    "    tweet = sent_segmenter.tokenize(tweet)\n",
    "    #put the words in a tweet into a words list\n",
    "    words = []\n",
    "    for sentence in tweet:\n",
    "        words= words+ word_tokenizer.tokenize(sentence)\n",
    "    #if the tweet has hashtags, add them to the words list too\n",
    "    if tag_list:\n",
    "        words=words+tag_list\n",
    "    if len(tag_list)>1:\n",
    "        if num_tweet_print<11:\n",
    "            print num_tweet_print\n",
    "            print ori_tweet\n",
    "            print words\n",
    "            num_tweet_print+=1\n",
    "\n",
    "    return words\n",
    "\n",
    "'''\n",
    "Given the file name, pre-process the file and return a list of preprocessed tweet \n",
    "and label lists\n",
    "'''\n",
    "def preprocess_file(filename):\n",
    "    tweets = []\n",
    "    labels=[]\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        tweets.append(preprocess(tweet_dict[\"text\"]))\n",
    "        labels.append(int(tweet_dict[\"label\"]))\n",
    "    return tweets,labels\n",
    "'''\n",
    "lemmatize a word. First treat the word as a verb, if not then treat the word as a noun\n",
    "'''\n",
    "def lemmatize(word):\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "7dc1fa62f9e403c31f91caba69d41d98516063cde6b0d1ced8de92df"
   },
   "source": [
    "<b>Instructions</b>: Once your basic preprocessing module is working, run it on the training set and have it print out 10 examples where your system identified a hashtag with more than one word inside; print out both the original tweet string as well as result after preprocessing. It's okay if you have to duplicate some code from above to do this. Point out any errors you see in the preprocessing, and discuss possible solutions; these can be related to the hashtags, or any other errors you see. You do not have to fix the errors unless they actually indicate a actual bug in your code (at which point you should go back to the previous section, fix the code, and print out the samples again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "signature": "81d5303fd413fafb373f77293320f3b0aca065df23c6f4a55043585e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @majornelson @Microsoft\n",
      "[u'if', u'i', u'make', u'a', u'game', u'as', u'a', u'universal', u'app', u'.', u'will', u'owners', u'be', u'able', u'to', u'download', u'and', u'play', u'it', u'in', u'november', u'?', u'windows', u'x', u'box', u'one']\n",
      "2\n",
      "@MikeWolf1980 @Microsoft I will be downgrading and let #Windows10 be out for almost the 1st yr b4 trying it again. #Windows10fail\n",
      "[u'i', u'will', u'be', u'downgrading', u'and', u'let', u'be', u'out', u'for', u'almost', u'the', u'1st', u'yr', u'b4', u'trying', u'it', u'again', u'.', u'windows10', u'windows10fail']\n",
      "3\n",
      "For the 1st time @Skype has a \"High Startup impact\" Does anyone at @Microsoft have a clue?#Windows10Fail http://t.co/loO3yd5rwe\n",
      "[u'for', u'the', u'1st', u'time', u'has', u'a', u'\"', u'high', u'startup', u'impact', u'\"', u'does', u'anyone', u'at', u'have', u'a', u'clue', u'?', u'windows10', u'fail']\n",
      "4\n",
      "#teens @BillGates 1st company failed miserably. When Gates & @PaulGAllen tried to sell the product it wouldn't work #nevergiveup @Microsoft\n",
      "[u'1st', u'company', u'failed', u'miserably', u'.', u'when', u'gates', u'&', u'tried', u'to', u'sell', u'the', u'product', u'it', u'wouldn', u\"'\", u't', u'work', u'teens', u'never', u'give', u'up']\n",
      "5\n",
      "#Vote for @AIESEC to become the 10th Global non profit partner of @Microsoft for us to #UpgradeYourWorld together. @AIESECGermany\n",
      "[u'for', u'to', u'become', u'the', u'10th', u'global', u'non', u'profit', u'partner', u'of', u'for', u'us', u'to', u'together', u'.', u'vote', u'upgrade', u'your', u'world']\n",
      "6\n",
      "Top 5 most searched for Back-to-School topics -- the list may surprise you http://t.co/Xj21uMVo0p  @bing @MSFTnews #backtoschool @Microsoft\n",
      "[u'top', u'5', u'most', u'searched', u'for', u'back', u'-', u'to', u'-', u'school', u'topics', u'--', u'the', u'list', u'may', u'surprise', u'you', u'back', u'to', u'school']\n",
      "7\n",
      "@taehongmin1 We have an IOT workshop by @Microsoft at 11PM on the Friday - definitely worth going for inspiration! #HackThePlanet\n",
      "[u'we', u'have', u'an', u'iot', u'workshop', u'by', u'at', u'11pm', u'on', u'the', u'friday', u'-', u'definitely', u'worth', u'going', u'for', u'inspiration', u'!', u'hack', u'the', u'planet']\n",
      "8\n",
      "@ForbesRussia #MBA #casestudy Namaste 2 #google and @Microsoft's CEOs, but #Multiculturals mttr!May B the era of Bad Translations wd B Over?\n",
      "[u'namaste', u'2', u'and', u'ceos', u',', u'but', u'mttr', u'!', u'may', u'b', u'the', u'era', u'of', u'bad', u'translations', u'wd', u'b', u'over', u'?', u'm', u'b', u'a', u'cases', u'tu', u'd', u'y', u'goo', u'g', u'l', u'e', u'multiculturals']\n",
      "9\n",
      "After 75 minutes of being on hold with @Microsoft in India 1-800-936-5700 \"Adrian\" wants to transfer my call again(3rd time) #Windows10Fail\n",
      "[u'after', u'75', u'minutes', u'of', u'being', u'on', u'hold', u'with', u'in', u'india', u'1', u'-', u'800', u'-', u'936', u'-', u'5700', u'\"', u'adrian', u'\"', u'wants', u'to', u'transfer', u'my', u'call', u'again', u'(', u'3rd', u'time', u')', u'windows10', u'fail']\n",
      "10\n",
      "We're excited to learn about #cloud #analytics from @Microsoft tomorrow! Join us https://t.co/p0bMREBBHC #tech #rva http://t.co/1XHmPdSvzq\n",
      "[u'we', u\"'\", u're', u'excited', u'to', u'learn', u'about', u'from', u'tomorrow', u'!', u'join', u'us', u'cloud', u'analytics', u'tech', u'r', u'v', u'a']\n"
     ]
    }
   ],
   "source": [
    "trn_tweets,trn_labels=preprocess_file('train.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>My analysis</b>:\n",
    "From the above output, we can see the following errors:\n",
    "1. When the hashtag has several words in it, but does not have enough capital letters to separate them, like  #Windows10fail, our code only split this hashtag based on \"W\", thus returning Windows10fail as a single word. \n",
    "2. When the hashtag is an abbreviation, consisting of only capital letters, like #MBA, which should be recognized as a single word, but our code would split it into \"M\",\"B\",\"A\"\n",
    "3. My implementation of maxMatch matches the hashtag from left to right, for hashtags like #casestudy, the algorithm would first match 'cases' with 'case', and 'tudy' is left, no word can be matched with 'tudy'\n",
    "4. Punctuations are useless most of the times but are not removed.\n",
    "\n",
    "\n",
    "To solve problem 1 and 2, we have to include a more delicate method to tell if the hashtag is an abbreviation, or if there is not enough capital letters to split the string. In addition, many words are not included in the list of English words from NLTK, thus we may need a more comprehensive lexicon. For error 3, a more intelligent maxMatch algorithm is needed, which will match the longest word in the string first, and is able to tell whether 's' or 'ed' come from the tense of the word. For error 4, we just need to include an extra step to remove punctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "ec5ec021ac23aa5b89b40b7df4d6c8031e8e07cae5e2eebf80d873d0"
   },
   "source": [
    "<b>Instructions</b>: The next step will be to convert each of your preprocessed tweets into a feature dictionary, that is, a python dictionary where each entry corresponds to a feature and its value. At this stage, you should just build a bag-of-word feature dict, though you must allow for two possible options: one is to remove stopwords (using the NLTK stopword list), and the other is to remove words appearing <em>less</em> than n times across the entire training set (n<=0 should have no effect). The outer function (convert_to_feature dicts) should take the list of tweets resulting from the preprocess_file, and return a list of feature dictionaries in the same order (so they correspond to the label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "signature": "dfc552b1cad57233536271c6d32c3f09dba50ff56286f6a194e25f90"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Method convert_to_feature_dicts\n",
    "Take the preprocessed list of tweets as input, remove_stop_words indicates whether to remove\n",
    "stop words, and n denotes whether to remove words appearing less than n times across the \n",
    "entire training set. \n",
    "Return a feature dictionary. \n",
    "\n",
    "First construct a dictionary using all the words in the training set, which is used to \n",
    "identify the words appearing less than n times. These words are stored into small_set. \n",
    "If remove_stop_words==True and n >0, then only words not in small_set and stop_word_set \n",
    "will be added to the feature dic.\n",
    "'''\n",
    "\n",
    "def convert_to_feature_dicts(tweets,remove_stop_words,n):\n",
    "    feature_dicts = []\n",
    "    if remove_stop_words:\n",
    "        stop_word_set=set(stopwords.words('english'))\n",
    "    if n>0:\n",
    "        small_set=set()\n",
    "        whole_feature_dic={}\n",
    "        for tweet in tweets:\n",
    "            for w in tweet:\n",
    "                whole_feature_dic[w]=whole_feature_dic.get(w,0)+1\n",
    "\n",
    "        for w in whole_feature_dic:\n",
    "            if whole_feature_dic[w]<=n:\n",
    "                small_set.add(w)\n",
    "\n",
    "    for tweet in tweets:\n",
    "        # build feature dictionary for tweet\n",
    "        feature_dict={}\n",
    "        for w in tweet:\n",
    "\n",
    "            if remove_stop_words and n<=0:\n",
    "                if w not in stop_word_set:\n",
    "                    feature_dict[w]=feature_dict.get(w,0)+1\n",
    "            elif remove_stop_words and n>0:\n",
    "                if w not in stop_word_set and w not in small_set:\n",
    "                    feature_dict[w]=feature_dict.get(w,0)+1\n",
    "            elif n>0:\n",
    "                if w not in small_set:\n",
    "                    feature_dict[w]=feature_dict.get(w,0)+1\n",
    "            else:\n",
    "                feature_dict[w]=feature_dict.get(w,0)+1\n",
    "\n",
    "        feature_dicts.append(feature_dict)\n",
    "\n",
    "    return feature_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "932fa4ffe864b9155e1d31b112ec463cd0c11684f77071cfeaea72f7"
   },
   "source": [
    "## Tuning and classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "00083e185628eaf2ebd60c081534bb750f4755b7a683f5c2dcea11ce"
   },
   "source": [
    "<b>Instructions</b>: Using the functions you've written, you should produce lists of feature dictionaries for both training and development sets; for the training set, remove stopwords and all words that appear only once (do <em>not</em> this for the dev set). Using scikit learn, convert the data to the sparse representation used for training classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "signature": "20ef700ae3f07171f04d1e3d5cdf837b36e17f9dcf1f80881b767a0d"
   },
   "outputs": [],
   "source": [
    "\n",
    "#proprocess training data\n",
    "trn_tweets,trn_labels=preprocess_file('train.json')\n",
    "# convert training data into feature dictionaries\n",
    "trn_feature_dicts=convert_to_feature_dicts(trn_tweets,True,1)\n",
    "#proprocess development data\n",
    "dev_tweets,dev_labels=preprocess_file('dev.json')\n",
    "# convert development data into feature dictionaries\n",
    "dev_feature_dicts=convert_to_feature_dicts(dev_tweets,True,0)\n",
    "\n",
    "#convert the data to the sparse representation\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "def prepare_data(trn_feature_dicts,dev_feature_dicts):\n",
    "    vectorizer = DictVectorizer()\n",
    "    trn_feature_dicts = vectorizer.fit_transform(trn_feature_dicts)\n",
    "    dev_feature_dicts = vectorizer.transform(dev_feature_dicts)\n",
    "    return trn_feature_dicts,dev_feature_dicts\n",
    "\n",
    "#convert training data and development data into sparse representation\n",
    "trn_feature_dicts,dev_feature_dicts=prepare_data(trn_feature_dicts,dev_feature_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e72f22d1037544244612b2150d94b99bfbf7eb86a789835e278d779f"
   },
   "source": [
    "<b>Instructions</b>: Now, tune a decision tree classifier using accuracy in the development set as the evaluation metric. For this, you need to consider at least 2 parameters of the model likely to influence performance and which make sense in this context; you should read the documentation for the classifier on sci-kit learn website to learn what these parameters are. For any binary or categorical parameters, you should just consider all options. For numerical values, you should start by keep other settings on default and just randomly try a wide range, looking for values above which there is a steep drop-off in performance, or, alternatively, no effect on performance at all (you don't need to show this process in the notebook).  Remember that some parameters should be tested on a logarithmic scale. Once you're fairly confident of a good range for the parameter, divide it up into at least 5 steps (but no more than 10), and carry out a grid search, which is to say an exhaustive exploration of all parameter options within the limits you've set (this should be included in the notebook). Identify the best parameter values, and discuss the influence of the parameters on performance in the development set. Do you think some values of the parameters are resulting in overfitting?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "signature": "97ad269db9c0e5026aeb2cadf02f4a0f27a0a2b85df3bf4c16720891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.455440131219\n",
      "0.463094587206\n",
      "0.455986878075\n",
      "0.460360852925\n",
      "0.459814106069\n",
      "0.459814106069\n",
      "0.453253143794\n",
      "0.45106615637\n",
      "0.461454346638\n",
      "0.452706396938\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMjOzwnKSMjOzwvr/a1LtZ3zJzZwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1187e1450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "'''\n",
    "Method test\n",
    "Return the accuracy of the classifier trained on the training data\n",
    "Input:\n",
    "clf: the classifier to be used\n",
    "training_data: the training data\n",
    "training_classifications: the labels of training data\n",
    "test_data: the data used to test the accuracy of the classifier trained on training data\n",
    "test_classifications: the labels of test data\n",
    "Return:\n",
    "Accuracy\n",
    "'''\n",
    "def test(clf,training_data,training_classifications,test_data,test_classifications):\n",
    "\n",
    "    clf.fit(training_data,training_classifications)\n",
    "    predictions = clf.predict(test_data)\n",
    "    accuracy = accuracy_score(test_classifications,predictions)\n",
    "    return accuracy\n",
    "\n",
    "'''\n",
    "Plot the accuracy results of decision tree classifier\n",
    "Input: \n",
    "accuracy_list is used to store the accuracies\n",
    "range_list indicates the list of numbers of min_samples_split, which is parameter for decision\n",
    "tree classifier\n",
    "'''\n",
    "def plot_graph(accuracy_list,range_list):\n",
    "    n_groups = 5\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.4\n",
    "    rects1 = plt.bar(index, accuracy_list[0:5], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='b',\n",
    "                     label='best')\n",
    "    rects2 = plt.bar(index + bar_width, accuracy_list[5:10], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='r',\n",
    "                     label='random')\n",
    "\n",
    "    plt.xlabel('Classifier parameters')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy using different classifier parameters')\n",
    "    plt.xticks(index + bar_width, range_list)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "accuracy_list=[]\n",
    "for split_method in ['best','random']:\n",
    "    for num_split in range(2,12,2):\n",
    "        clf1=DecisionTreeClassifier(splitter=split_method,min_samples_split=num_split)\n",
    "\n",
    "        acc= test(clf1,trn_feature_dicts,trn_labels,dev_feature_dicts,dev_labels)\n",
    "        print acc\n",
    "        accuracy_list.append(acc)\n",
    "        \n",
    "plot_graph(accuracy_list,range(2,12,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "3586fbaaae10742a1f14cf65912fd539eb723b13e25d419ea1f91f49"
   },
   "source": [
    "<b>Instructions</b>: Carry out the same tuning process with the logistic regression classifier. Compare the performance of the two classifiers to each other, and to the most common class baseline. How are the classifiers doing? Is this a challenging task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "73eed72d2b12d09b05c29be321905bf8eaad8b8c009e3586dca40162"
   },
   "outputs": [],
   "source": [
    "clf2=LogisticRegression()\n",
    "\n",
    "print test(clf2,trn_feature_dicts,trn_labels,dev_feature_dicts,dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "88bc514c88161b16a2531906d60a0851ce0d940169b86ba3d7414918"
   },
   "source": [
    "<b>Instructions</b>: The next task is a slight detour to test your understanding of the logistic regression classifier: you are going to build your own classifier based on the trained model from sci-kit learn. In particular, you should fill in the MyLogisticRegression class started below which is initialized using the feature weights (coefficients) and constants (intercepts) and list of labels (classes) from the sci-kit learn classifier (see the \"Attributes\" in the documentation for the Logistic Regression classifier), and which mimics the predict and predict_proba methods from the sci-kit learn classifier object. You should confirm that your solution works by using it in the task at hand: take the classifier defined below, train it on the training data, then create an instance of MyLogisticRegression, and show that your classifier has the same output as the scikit-learn classifier for both predict and predict_proba for 5 samples from the development set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "signature": "81a3035ebbbd8625ede32260be722cf4ca25d8df6d80fab8578017fa"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d7b5e9b38347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# train the classifer here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "class MyLogisticRegression:\n",
    "    \n",
    "    def __init__(self, weights, constants, labels):\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        pass\n",
    "    \n",
    "    def predict(self,X):\n",
    "        pass\n",
    "    \n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# train the classifer here\n",
    "\n",
    "my_clf = MyLogisticRegression(clf.coef_, clf.intercept_, clf.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "7eb4249c43c16b198796115b2b1977a30e1f32cf0e8efde44391938f"
   },
   "source": [
    "## Polarity Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "222499934251174c01259d47f9aa0b77fc082c539f3ec8fc74293fb6"
   },
   "source": [
    "<b>Instructions</b>: Next we will try integrating information from sources beyond the training set, in the form of polarity lexicons. The main focus of this section is producing and evaluating 3 automatically-built polarity lexicons. The first of these lexicons is SentiWordNet, which is <a href=\"http://www.nltk.org/howto/sentiwordnet.html\"> accessible through NLTK</a>. SentiWordNet has precalculated scores for positive, negative, and neutral sentiment for some of the words in WordNet, but, like WordNet, it is arranged in synsets; building a WSD system to handle this is beyond the scope of this assignment, instead you should take the most common polarity across its senses (neutral if there is a tie). Do this by iterating through all the synsets in WordNet (which may take a little while, the code snippet below has a counter to show your progress), and then create two lists, one of positive words, one of negative words. Show 5 examples of each of the positive and negative words, and comment on their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "89220443510ad7c018f80daf991ee34cba839e7b5eaaff0adf64ebf8"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "\n",
    "def get_polarity_type(synset_name):\n",
    "    swn_synset =  swn.senti_synset(synset_name)\n",
    "    if not swn_synset:\n",
    "        return None\n",
    "    elif swn_synset.pos_score() > swn_synset.neg_score() and swn_synset.pos_score() > swn_synset.obj_score():\n",
    "        return 1\n",
    "    elif swn_synset.neg_score() > swn_synset.pos_score() and swn_synset.neg_score() > swn_synset.obj_score():\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "for synset in wn.all_synsets():\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print count\n",
    "    # count synset polarity for each lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9ce3937489fc060ba10b096aae24b2b953c9fd104f244c92c97c82e0"
   },
   "source": [
    "<b>Instructions</b>: The second lexicon will be built using the word2vec (CBOW) vectors included in NLTK. For this, you will need a small set of positive and negative seed terms, which are given to you below. Calculate cosine similarity between vectors of the seeds terms and each of the words for which you have vectors (if you use Gensim, you can iterate over model.vocab), flip the sign for the negative seeds, and then average to get a score. Use this score to produce a list of positive and negative words; you should include a threshold of Â±0.03 for words to be considered positive or negative. Again, show 5 examples of each of the positive and negative words, and comment on their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "9931c8c4464e5da7da0b9b5a2fea8e9cd12a5a5fcc8935a40c27ee4e"
   },
   "outputs": [],
   "source": [
    "positive_seeds = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\",\"great\"]\n",
    "negative_seeds = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\",\"awful\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "0bae4e130bdd5a1c767517625a7f0b686e7fb27448d03a3f41358314"
   },
   "source": [
    "<b>Instructions</b>: The third lexicon will be built by calculating PPMI with the seed terms. For this, use the Brown corpus included in NLTK, with co-occurrence defined as <em>binary</em> text co-occurrence (that is, multiple co-occurrences in the same text are not counted); importantly, your solution should <em>not</em> calculate the entire co-occurrence matrix, since you only care about relative co-occurrence with the seeds. As above, average the resulting similarity scores after switching the sign for the negative seeds and use them to produce a list of positive and negative words, and check 5 of each. For PPMI, use a threshold of  Â±0.3 for deciding if a word is neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "6dd1b963e2f08f3169f207703e2558f8f3b28e42f002125372f43d6a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "df2905143fcc1c2614933587329996421e16ea24a28c95f81d8c1a39"
   },
   "source": [
    "<b>Instructions</b>: Now you will test these automatically-produced lexicons against a manually-annotated set. There is a manually-built lexicon (the Hu and Liu lexicon) which is included with NLTK. It has a list of positive and negative words, which are accessed as below. First, investigate what percentage of the words in the manual lexicon are in each of the automatic lexicons, and then, only for those words which overlap and which are <em>not</em> in the seed set, evaluate the accuracy of with each of the automatic lexicons. Discuss the results, mentioning why you think the lexicon which won out did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "a21792bdaddd795fa32b1dc88bfdd995cd5849f8a0e06a09071b09e2"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "4c379c7fd57895c160f0dce018da0db65fceef7f344a669a01c9cabe"
   },
   "source": [
    "<b>Instructions</b>: Now you will use the lexicons (both manual and automatic) for the main classification problem. Create a function which calculates a polarity score for a sentence based on a given lexicon (i.e. counting positive and negative words that appear in the tweet, and then returning +1 if there are more positive words, -1 if there are more negative words, and 0 otherwise). Then, use this to compare the results of the different lexicons (please convert them to sets!) on the task in the development set, i.e. the accuracy relative to the human-annotated labels. Do the results reflect the quality of the lexicon as indicated by the earlier analysis? How does it compare to the logistic regression classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "ebef05138e316018347a5747092dc11105a230d38770462954d39a2d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "a5b0e40cf431f6ecb3e0e18d6b2ab9213ddcd1198d1cb6bfa0642dbc"
   },
   "source": [
    "<b>Instructions</b>: Now you should investigate the effect of adding the polarity score (or scores) as a feature in your statistical classifier. You should create a new version of your convert_to_feature_dict function (with a different name) to include the extra feature (or features), do not modify the code in that earlier section directly. Retrain your best logistic regression classifier from the early tuning, test on the development set. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "f9bf187f20f9c509f47c8b184038fa011048d11feca82ca91b40a55f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "5ffc558321e254f8a6893e5affb98867a893b1ffbab8a813756b6e71"
   },
   "source": [
    "## Error analysis and improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e140733186799a3e75492cb90d87bc53e639ef62db47c93afffb4cf2"
   },
   "source": [
    "<b>Instructions</b>: Using your best logistic regression classifier so far, first write a function to identify errors your classifier is making where the probability of the predicted class and the actual class are fairly close (less than 0.2); you're looking for cases which you have a good chance of getting right with a small improvement. For this, do an 80/20 split of  the training dataset (that is, train on 80% of the data, test on 20%); do not look at examples from the development set or the test set. You should print out the tweet, the correct class, the predicted class, and the probabilities. Don't print all the errors, just use random.sample to select 30 from the full set. Look for general patterns in the errors, and propose a reasonable improvement to your classifier that you think might help with a problem that you are seeing. It could involve, for instance, better preprocessing, the addition of new features, some kind of feature selection, better lexicons or better use of the lexicons, or even a post-processing step. It should not require additional data, unless it involves a small set of words that you can hardcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "d93d0f3cdc8a2acbb253fe0c200fe8421d8d15a6346c8b239dedafcd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "03077b4b348fb16cab9f35475a99a16df22012d2107d4592d7b9a7d3"
   },
   "source": [
    "<b>Instructions</b>: Now implement that improvement, and then investigate its effect <em>in the development data</em>. Obviously, different improvements may involve different amounts of effort; if your improvement is fairly simple, we expect that you will do a more in-depth analysis, testing possible variations. You can also do multiple related improvements. Students who put extra effort into this may get few extra points that can offset any mistakes on other parts of the assignment, though we do not recommend you spend extra time on this before the other parts of the assignment are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "ab77e3b674fbb64ad43b2f03bdcc66cd9cffed98de849fd115a44072"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "2b0f4d9e8ab0fe8d28e48b8dba46d891745a82101b77551f31f52ff2"
   },
   "source": [
    "## Final testing and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "c56e9bb9790acf94081360a67b83dc79b7a5a96f159c21d2ccf2f9d3"
   },
   "source": [
    "<b>Instructions</b>: When the final test set has been released, you should start with your best classifier from your work up to this point, and do a final test of all major options, including at least one from each of the first four sections of the assignment (that is, at least one preprocessing option, at least one tuning parameter/classifier type, at least one lexicon, and your improvement), in this new dataset. You don't need to explore every possible combination (in fact, you shouldn't), but you should make a convincing case that you have probably found the best combination given the possibilities you have implemented. It's okay if you find discrepancies between the best classifier on the test set and development set, just be sure to mention them. In a final discussion (which should be at least 500 words), you should include at least one bar graph of accuracy across various options, and at least one table which reports precision, recall, and F-score for each label as well as the macroaveraged F-score (all figures and tables should be generated inline by your code, using matplotlib). Please conclude your discussion by discussing what you have learned, and mentioning any other ideas for improving performance of this system that you may have.\n",
    "\n",
    "Note that you may have to direct matplotlib to display the figures inline, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "8c26ed656bdee89c90809063e6a99a73f200067917ed00f4de13cb84"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
